---
title: "Life Expectancy Progress Memo 1"
subtitle: "Data Science 2 with R (STAT 301-2)"
author: "Alice Meng, Nina Meng, Stuart Sumner, Emiliano Ghislieri"

format:
  html:
    toc: true
    toc-depth: 4
    embed-resources: true
    echo: false
    link-external-newwindow: true
    
execute:
  warning: false
  message: false
  
from: markdown+emoji 
---

```{r}
# set seed
set.seed(4444)
# load packages + data
library(ggcorrplot)
library(tidyverse)
library(tidymodels)
tidymodels_prefer()
load("initial_setup/tuning_setup.rda")

```

## Link to Github Repo

[Github Repo Link](https://github.com/STAT301-3-2023SP/final-project-datatists)

## Explanation of the Prediction Problem

We will be predicting life expectancy for the year 2015 using data from life expectancies of previous years (2000-2014) as well as predictors that can affect life expectancy, such as region, illness prevalence, and economic status. This will be a regression problem, with [Life Expectancy data](https://www.kaggle.com/datasets/lashagoch/life-expectancy-who-updated) from WHO.

## Overview of the Dataset

There are a total of 3043 observations with 19 predictor variables. The variable types include numeric for variables such as adult mortality, Hepatitis-B immunization rates, and schooling, as well as factor variables for geographic region and binary variables for economic status (whether the countries are developed or developing).

This data is originally formatted as a tidy table of average life expectancy by country and year with corresponding health measures. To avoid using a time-series analysis, we rearranged the data such that each country and year contained the average life expectancy from that year as well as the following year's average life expectancy. For example, the USA data from 2013 would contain both the average life expectancy in 2013 and the average life expectancy from 2014. This allows us to use the following year's life expectancy as our outcome variable (and the current year's as a predictor).

The target variable will be average life expectancy in 2015. The data was split into training and testing datasets, with the current year's life expectancy from 2000-2013 in the training data, and those from 2014 in the testing data. Because of how we arranged our data, our model will be trained to predict the following year's life expectancy â€” because we have only up to 2015's actual values, we will be assessing how well our model does in predicting the life expectancy following 2014.


### Missingness Assessment

```{r}
life_train %>% 
  skimr::skim_without_charts(following_life_expect)
```

The data is relatively clean, and there is no missing data.

### Collinearity Assessment

```{r}

corrplotdata = life_train %>% 
  select(-1, -19)

corr = round(cor(corrplotdata), 1)
ggcorrplot(corr)
```

We conducted a collinearity assessment to ensure no two variables were correlated past a threshold of 0.7. A number of variables have high correlations, such as under_five_deaths and infant_deaths. Thus, we created a second recipe with step_corr with a correlation threshold of 0.7, which would remove the variables with a correlation coefficient higher than 0.7. We will use this second recipe on our best performing models after running the basic first recipe to see if model performance further improves.

### Target Variable Distribution

```{r}

life_train %>% 
  ggplot(aes(following_life_expect)) +
  geom_histogram() +
  theme_minimal()
```

The distribution in our target variable (2015 life expectancy) peaks at around 70-80 years, before significantly decreasing at \~83 years.

### Model Comparison & Final Model Evaluation

We will be using the following models: boosted tree, elastic net, k-nearest-neighbors, logistic model, MARS, neural network, random forest, SVM polynomial, and SVM radial. For resampling, we split the data into 10 folds with 3 repeats, which provides a good sampling of data to work with without extending model run times significantly. Our evaluation metrics will likely be RMSE and R^2 and MAE, with RMSE as the initial standard of comparison for model selection. After we run the evaluation metrics on all of our models and identify the two best performing models, we will fit that model to the testing dataset and add the second recipe to see if it further improves performance.
